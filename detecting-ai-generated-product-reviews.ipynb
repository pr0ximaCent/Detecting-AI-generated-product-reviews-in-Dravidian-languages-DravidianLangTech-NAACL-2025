{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10340431,"sourceType":"datasetVersion","datasetId":6403026}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nimport torch\nimport os\nimport shutil\n\n# Clear previous outputs\nfor folder in [\"./results\", \"./logs\", \"./submission\"]:\n    if os.path.exists(folder):\n        shutil.rmtree(folder)\n\n# Disable Weights & Biases integration\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Text preprocessing with advanced cleaning\ndef preprocess_text(text):\n    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n    return text\n\n# Paths for Kaggle datasets\ntrain_paths = {\n    \"tamil\": \"/kaggle/input/dataset/tam_training_data_hum_ai.csv\",\n    \"malayalam\": \"/kaggle/input/dataset/mal_training_data_hum_ai.csv\"\n}\ntest_paths = {\n    \"tamil\": \"/kaggle/input/dataset/tam_test_data_hum_ai.xlsx\",\n    \"malayalam\": \"/kaggle/input/dataset/mal_test_data_hum_ai.xlsx\"\n}\n\n# Verify files\nfor path in train_paths.values():\n    assert os.path.exists(path), f\"Training file not found: {path}\"\nfor path in test_paths.values():\n    assert os.path.exists(path), f\"Test file not found: {path}\"\n\n# Load datasets\ntrain_data = {lang: pd.read_csv(path) for lang, path in train_paths.items()}\ntest_data = {lang: pd.read_excel(path) for lang, path in test_paths.items()}\n\n# Validate required columns\nfor lang, df in train_data.items():\n    assert 'DATA' in df.columns and 'LABEL' in df.columns, f\"Missing columns in {lang} training data\"\nfor lang, df in test_data.items():\n    assert 'DATA' in df.columns, f\"Missing 'DATA' column in {lang} test data\"\n\n# Preprocess datasets\nfor lang, df in train_data.items():\n    df['CLEANED_DATA'] = df['DATA'].apply(preprocess_text)\n\n# Tokenizer and Model\nMODEL_NAME = \"microsoft/deberta-v3-large\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"CLEANED_DATA\"], padding=\"max_length\", truncation=True, max_length=512)\n\n# Split datasets into training and validation\nencoded_datasets = {}\nfor lang, df in train_data.items():\n    df['LABEL'] = df['LABEL'].map({\"HUMAN\": 0, \"AI\": 1})\n    train_split, val_split = train_test_split(df, test_size=0.2, stratify=df['LABEL'], random_state=42)\n    encoded_datasets[lang] = (train_split, val_split)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",  # Match eval and save strategy\n    save_total_limit=1,  # Retain only the best checkpoint\n    learning_rate=2e-5,  # Optimized learning rate\n    per_device_train_batch_size=4,  # Optimize batch size\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=8,  # Maintain effective batch size\n    num_train_epochs=5,  # Reduce epochs\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=500,  # Log less frequently\n    fp16=True,  # Enable mixed precision training\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=500,\n    label_smoothing_factor=0.2,\n    report_to=[\"none\"],  # Disable external logging\n)\n\nmodels = {}\nfor lang, (train_split, val_split) in encoded_datasets.items():\n    train_encodings = tokenizer(list(train_split['CLEANED_DATA']), truncation=True, padding=True, max_length=512)\n    val_encodings = tokenizer(list(val_split['CLEANED_DATA']), truncation=True, padding=True, max_length=512)\n\n    class Dataset(torch.utils.data.Dataset):\n        def __init__(self, encodings, labels):\n            self.encodings = encodings\n            self.labels = labels\n\n        def __len__(self):\n            return len(self.labels)\n\n        def __getitem__(self, idx):\n            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n            return item\n\n    train_dataset = Dataset(train_encodings, list(train_split['LABEL']))\n    val_dataset = Dataset(val_encodings, list(val_split['LABEL']))\n\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=lambda p: {\"f1\": f1_score(np.argmax(p.predictions, axis=1), p.label_ids, average=\"macro\")},\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n    )\n    trainer.train()\n\n    # Store the trained model\n    models[lang] = model\n\n    # Clear intermediate results\n    del train_dataset, val_dataset, train_split, val_split, train_encodings, val_encodings\n    torch.cuda.empty_cache()\n\n# Predictions for test data\nsubmission = {}\nfor lang, df in test_data.items():\n    df['CLEANED_DATA'] = df['DATA'].apply(preprocess_text)\n    test_encodings = tokenizer(list(df['CLEANED_DATA']), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n    logits = models[lang](**test_encodings).logits\n    preds = torch.argmax(logits, axis=1).numpy()\n    df['PREDICTION'] = preds\n    df['PREDICTION'] = df['PREDICTION'].map({0: \"HUMAN\", 1: \"AI\"})\n    submission[lang] = df[['ID', 'PREDICTION']]\n\n# Save submission\nos.makedirs(\"submission\", exist_ok=True)\nTEAM_NAME = \"CUET_NetworkSociety\"\nfor lang, sub_df in submission.items():\n    sub_df.to_csv(f\"submission/{TEAM_NAME}_{lang}_run.tsv\", sep=\"\\t\", index=False)\n\n# Create the zip file in the Kaggle working directory\nshutil.make_archive(f\"/kaggle/working/{TEAM_NAME}\", 'zip', \"submission\")\n\n# Debug: Verify output directory\nprint(\"Submission Directory Contents:\", os.listdir(\"submission\"))\nprint(\"Root Directory Contents:\", os.listdir(\"/kaggle/working\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}